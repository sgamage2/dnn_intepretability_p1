{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wudan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 newsgroup EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n",
      "(11314,) (11314,)\n",
      "[ 7  4  4  1 14 16 13  3  2  4  8 19  4 14  6  0  1  7 12  5]\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names))\n",
    "print(newsgroups_train.filenames.shape,newsgroups_train.target.shape)\n",
    "print(newsgroups_train.target[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 34118)\n",
      "159.0132743362832\n",
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "(2034,) (2034,)\n",
      "[1 3 2 0 2 0 2 1 2 1 2 1 1 2 1 2 0 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "cats = ['alt.atheism', 'talk.religion.misc',\n",
    "...               'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(vectors.shape)\n",
    "print(vectors.nnz / float(vectors.shape[0]))\n",
    "\n",
    "print(list(newsgroups_train.target_names))\n",
    "print(newsgroups_train.filenames.shape,newsgroups_train.target.shape)\n",
    "print(newsgroups_train.target[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7731035068127478\n",
      "alt.atheism: edu it and in you that is of to the\n",
      "comp.graphics: edu in graphics it is for and of to the\n",
      "sci.space: edu it that is in and space to of the\n",
      "talk.religion.misc: not it you in is that and to of the\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'),categories=cats)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "print(metrics.f1_score(newsgroups_test.target, pred, average='macro'))\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "print(show_top10(clf, vectorizer, newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to TF-IDF:\n",
    "def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
    "    return (X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n",
    "    \"\"\"\n",
    "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
    "    Build Deep neural networks Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The split between the train and test set is based upon messages posted before and after a specific date.\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 75000 features\n",
      "WARNING:tensorflow:From /home/wudan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/10\n",
      " - 30s - loss: 2.7973 - accuracy: 0.1015 - val_loss: 2.0871 - val_accuracy: 0.2860\n",
      "Epoch 2/10\n",
      " - 29s - loss: 1.4579 - accuracy: 0.4731 - val_loss: 1.0952 - val_accuracy: 0.6377\n",
      "Epoch 3/10\n",
      " - 29s - loss: 0.7107 - accuracy: 0.7302 - val_loss: 0.9593 - val_accuracy: 0.7245\n",
      "Epoch 4/10\n",
      " - 30s - loss: 0.3936 - accuracy: 0.8616 - val_loss: 0.8837 - val_accuracy: 0.7689\n",
      "Epoch 5/10\n",
      " - 30s - loss: 0.2090 - accuracy: 0.9296 - val_loss: 0.9680 - val_accuracy: 0.7657\n",
      "Epoch 6/10\n",
      " - 30s - loss: 0.1273 - accuracy: 0.9598 - val_loss: 0.9333 - val_accuracy: 0.7958\n",
      "Epoch 7/10\n",
      " - 30s - loss: 0.0964 - accuracy: 0.9717 - val_loss: 0.9770 - val_accuracy: 0.7934\n",
      "Epoch 8/10\n",
      " - 30s - loss: 0.0645 - accuracy: 0.9827 - val_loss: 0.9310 - val_accuracy: 0.8014\n",
      "Epoch 9/10\n",
      " - 30s - loss: 0.0541 - accuracy: 0.9862 - val_loss: 0.9296 - val_accuracy: 0.8088\n",
      "Epoch 10/10\n",
      " - 30s - loss: 0.0414 - accuracy: 0.9901 - val_loss: 0.9490 - val_accuracy: 0.8067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       319\n",
      "           1       0.57      0.78      0.66       389\n",
      "           2       0.71      0.68      0.69       394\n",
      "           3       0.63      0.77      0.69       392\n",
      "           4       0.87      0.80      0.83       385\n",
      "           5       0.88      0.69      0.77       395\n",
      "           6       0.65      0.84      0.73       390\n",
      "           7       0.88      0.85      0.87       396\n",
      "           8       0.95      0.93      0.94       398\n",
      "           9       0.94      0.95      0.94       397\n",
      "          10       0.98      0.95      0.96       399\n",
      "          11       0.95      0.89      0.92       396\n",
      "          12       0.75      0.68      0.71       393\n",
      "          13       0.91      0.72      0.81       396\n",
      "          14       0.95      0.85      0.90       394\n",
      "          15       0.87      0.88      0.88       398\n",
      "          16       0.80      0.81      0.80       364\n",
      "          17       0.97      0.90      0.93       376\n",
      "          18       0.73      0.67      0.70       310\n",
      "          19       0.54      0.68      0.61       251\n",
      "\n",
      "    accuracy                           0.81      7532\n",
      "   macro avg       0.81      0.80      0.80      7532\n",
      "weighted avg       0.82      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)\n",
    "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 20)\n",
    "model_DNN.fit(X_train_tfidf, y_train,\n",
    "                              validation_data=(X_test_tfidf, y_test),\n",
    "                              epochs=10,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "predicted = model_DNN.predict(X_test_tfidf)\n",
    "print(metrics.classification_report(y_test, predicted.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               38400512  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 39,461,396\n",
      "Trainable params: 39,461,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_DNN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, GRU, Embedding\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"/home/wudan/SequenceClassification/glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    \"\"\"\n",
    "    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    word_index in word index ,\n",
    "    embeddings_index is embeddings index, look at data_helper.py\n",
    "    nClasses is number of classes,\n",
    "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    hidden_layer = 3\n",
    "    gru_node = 256\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
    "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
    "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    print(gru_node)\n",
    "    for i in range(0,hidden_layer):\n",
    "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(nclasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n",
      "(18846, 500)\n",
      "Total 400000 word vectors.\n",
      "256\n",
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/2\n",
      " - 649s - loss: nan - accuracy: 0.0440 - val_loss: nan - val_accuracy: 0.0424\n",
      "Epoch 2/2\n",
      " - 649s - loss: nan - accuracy: 0.0424 - val_loss: nan - val_accuracy: 0.0424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      1.00      0.08       319\n",
      "           1       0.00      0.00      0.00       389\n",
      "           2       0.00      0.00      0.00       394\n",
      "           3       0.00      0.00      0.00       392\n",
      "           4       0.00      0.00      0.00       385\n",
      "           5       0.00      0.00      0.00       395\n",
      "           6       0.00      0.00      0.00       390\n",
      "           7       0.00      0.00      0.00       396\n",
      "           8       0.00      0.00      0.00       398\n",
      "           9       0.00      0.00      0.00       397\n",
      "          10       0.00      0.00      0.00       399\n",
      "          11       0.00      0.00      0.00       396\n",
      "          12       0.00      0.00      0.00       393\n",
      "          13       0.00      0.00      0.00       396\n",
      "          14       0.00      0.00      0.00       394\n",
      "          15       0.00      0.00      0.00       398\n",
      "          16       0.00      0.00      0.00       364\n",
      "          17       0.00      0.00      0.00       376\n",
      "          18       0.00      0.00      0.00       310\n",
      "          19       0.00      0.00      0.00       251\n",
      "\n",
      "    accuracy                           0.04      7532\n",
      "   macro avg       0.00      0.05      0.00      7532\n",
      "weighted avg       0.00      0.04      0.00      7532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wudan/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 20)\n",
    "model_RNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=2,\n",
    "                              batch_size=64,\n",
    "                              verbose=2)\n",
    "predicted = model_RNN.predict_classes(X_test_Glove)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 50)           8960500   \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 500, 256)          235776    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "gru_18 (GRU)                 (None, 500, 256)          393984    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 500, 256)          393984    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 10,383,368\n",
      "Trainable params: 10,383,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_RNN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\n",
    "from keras.models import Sequential,Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"/home/wudan/SequenceClassification/glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    \"\"\"\n",
    "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "        word_index in word index ,\n",
    "        embeddings_index is embeddings index, look at data_helper.py\n",
    "        nClasses is number of classes,\n",
    "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
    "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    # applying a more complex convolutional approach\n",
    "    convs = []\n",
    "    filter_sizes = []\n",
    "    layer = 5\n",
    "    print(\"Filter  \",layer)\n",
    "    for fl in range(0,layer):\n",
    "        filter_sizes.append((fl+2))\n",
    "    node = 128\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        #l_pool = Dropout(0.25)(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
    "    l_cov1 = Dropout(dropout)(l_cov1)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
    "    l_cov2 = Dropout(dropout)(l_cov2)\n",
    "    l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "    l_flat = Flatten()(l_pool2)\n",
    "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    l_dense = Dense(512, activation='relu')(l_dense)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n",
      "(18846, 500)\n",
      "Total 400000 word vectors.\n",
      "Filter   5\n",
      "WARNING:tensorflow:From /home/wudan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 50)      8960500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 499, 128)     12928       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 498, 128)     19328       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 497, 128)     25728       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 496, 128)     32128       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 495, 128)     38528       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 99, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 99, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 99, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 99, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 99, 128)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 495, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 491, 128)     82048       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 491, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 98, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 94, 128)      82048       max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 94, 128)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 3, 128)       0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         394240      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          524800      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 512)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 20)           10260       dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,182,536\n",
      "Trainable params: 10,182,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/15\n",
      " - 34s - loss: 2.9700 - accuracy: 0.0760 - val_loss: 2.8005 - val_accuracy: 0.1415\n",
      "Epoch 2/15\n",
      " - 33s - loss: 2.3042 - accuracy: 0.2025 - val_loss: 2.3016 - val_accuracy: 0.3079\n",
      "Epoch 3/15\n",
      " - 33s - loss: 1.7535 - accuracy: 0.3502 - val_loss: 1.9884 - val_accuracy: 0.3991\n",
      "Epoch 4/15\n",
      " - 33s - loss: 1.3842 - accuracy: 0.4875 - val_loss: 1.6523 - val_accuracy: 0.5392\n",
      "Epoch 5/15\n",
      " - 33s - loss: 1.0625 - accuracy: 0.6116 - val_loss: 1.4470 - val_accuracy: 0.6049\n",
      "Epoch 6/15\n",
      " - 33s - loss: 0.8162 - accuracy: 0.7051 - val_loss: 1.2994 - val_accuracy: 0.6472\n",
      "Epoch 7/15\n",
      " - 33s - loss: 0.6319 - accuracy: 0.7780 - val_loss: 1.1912 - val_accuracy: 0.6402\n",
      "Epoch 8/15\n",
      " - 33s - loss: 0.4979 - accuracy: 0.8227 - val_loss: 1.0964 - val_accuracy: 0.6879\n",
      "Epoch 9/15\n",
      " - 33s - loss: 0.3948 - accuracy: 0.8613 - val_loss: 0.9939 - val_accuracy: 0.6960\n",
      "Epoch 10/15\n",
      " - 33s - loss: 0.3120 - accuracy: 0.8931 - val_loss: 0.9598 - val_accuracy: 0.7074\n",
      "Epoch 11/15\n",
      " - 33s - loss: 0.2393 - accuracy: 0.9173 - val_loss: 0.9177 - val_accuracy: 0.7090\n",
      "Epoch 12/15\n",
      " - 33s - loss: 0.1918 - accuracy: 0.9353 - val_loss: 0.9098 - val_accuracy: 0.7168\n",
      "Epoch 13/15\n",
      " - 33s - loss: 0.1617 - accuracy: 0.9470 - val_loss: 0.8643 - val_accuracy: 0.7319\n",
      "Epoch 14/15\n",
      " - 33s - loss: 0.1157 - accuracy: 0.9631 - val_loss: 0.8664 - val_accuracy: 0.7292\n",
      "Epoch 15/15\n",
      " - 33s - loss: 0.0962 - accuracy: 0.9696 - val_loss: 0.8941 - val_accuracy: 0.7191\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-896e2ee2d270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_Glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
    "model_CNN.summary()\n",
    "model_CNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "predicted = model_CNN.predict(X_test_Glove)\n",
    "predicted = np.argmax(predicted, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       319\n",
      "           1       0.63      0.69      0.66       389\n",
      "           2       0.71      0.58      0.64       394\n",
      "           3       0.51      0.61      0.56       392\n",
      "           4       0.47      0.75      0.57       385\n",
      "           5       0.64      0.65      0.64       395\n",
      "           6       0.77      0.74      0.75       390\n",
      "           7       0.72      0.79      0.76       396\n",
      "           8       0.96      0.62      0.75       398\n",
      "           9       0.90      0.91      0.91       397\n",
      "          10       0.98      0.90      0.94       399\n",
      "          11       0.76      0.91      0.83       396\n",
      "          12       0.54      0.47      0.50       393\n",
      "          13       0.85      0.77      0.81       396\n",
      "          14       0.80      0.82      0.81       394\n",
      "          15       0.82      0.85      0.84       398\n",
      "          16       0.71      0.77      0.74       364\n",
      "          17       0.93      0.87      0.90       376\n",
      "          18       0.67      0.48      0.56       310\n",
      "          19       0.45      0.33      0.38       251\n",
      "\n",
      "    accuracy                           0.72      7532\n",
      "   macro avg       0.73      0.71      0.71      7532\n",
      "weighted avg       0.73      0.72      0.72      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n",
      "(18846, 500)\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"/home/wudan/SequenceClassification/glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n",
    "def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):\n",
    "    kernel_size = 2\n",
    "    filters = 256\n",
    "    pool_size = 2\n",
    "    gru_node = 256\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1024,activation='relu'))\n",
    "    model.add(Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter   5\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 500, 50)      8960500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 499, 128)     12928       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 498, 128)     19328       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 497, 128)     25728       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 496, 128)     32128       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 495, 128)     38528       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 99, 128)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 99, 128)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 99, 128)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 99, 128)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 99, 128)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 495, 128)     0           max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "                                                                 max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 491, 128)     82048       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 491, 128)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 98, 128)      0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 94, 128)      82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 94, 128)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 3, 128)       0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1024)         394240      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1024)         0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 512)          524800      dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 512)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 20)           10260       dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,182,536\n",
      "Trainable params: 10,182,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/15\n",
      " - 34s - loss: 2.9645 - accuracy: 0.0732 - val_loss: 2.8329 - val_accuracy: 0.1156\n",
      "Epoch 2/15\n",
      " - 33s - loss: 2.3268 - accuracy: 0.1896 - val_loss: 2.3413 - val_accuracy: 0.2910\n",
      "Epoch 3/15\n",
      " - 33s - loss: 1.7642 - accuracy: 0.3457 - val_loss: 1.9199 - val_accuracy: 0.4278\n",
      "Epoch 4/15\n",
      " - 33s - loss: 1.3435 - accuracy: 0.5007 - val_loss: 1.6330 - val_accuracy: 0.5397\n",
      "Epoch 5/15\n",
      " - 34s - loss: 1.0395 - accuracy: 0.6165 - val_loss: 1.3982 - val_accuracy: 0.6065\n",
      "Epoch 6/15\n",
      " - 34s - loss: 0.8236 - accuracy: 0.7062 - val_loss: 1.2583 - val_accuracy: 0.6499\n",
      "Epoch 7/15\n",
      " - 34s - loss: 0.6213 - accuracy: 0.7773 - val_loss: 1.1523 - val_accuracy: 0.6709\n",
      "Epoch 8/15\n",
      " - 34s - loss: 0.4918 - accuracy: 0.8260 - val_loss: 1.0915 - val_accuracy: 0.6873\n",
      "Epoch 9/15\n",
      " - 34s - loss: 0.3756 - accuracy: 0.8674 - val_loss: 0.9812 - val_accuracy: 0.6981\n",
      "Epoch 10/15\n",
      " - 34s - loss: 0.2917 - accuracy: 0.8976 - val_loss: 0.9614 - val_accuracy: 0.7065\n",
      "Epoch 11/15\n",
      " - 34s - loss: 0.2365 - accuracy: 0.9167 - val_loss: 0.9257 - val_accuracy: 0.7112\n",
      "Epoch 12/15\n",
      " - 34s - loss: 0.1823 - accuracy: 0.9386 - val_loss: 0.8444 - val_accuracy: 0.7390\n",
      "Epoch 13/15\n",
      " - 34s - loss: 0.1656 - accuracy: 0.9467 - val_loss: 0.8911 - val_accuracy: 0.7220\n",
      "Epoch 14/15\n",
      " - 34s - loss: 0.1356 - accuracy: 0.9558 - val_loss: 0.8451 - val_accuracy: 0.7428\n",
      "Epoch 15/15\n",
      " - 35s - loss: 0.0959 - accuracy: 0.9688 - val_loss: 0.8235 - val_accuracy: 0.7381\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c29dfae2274f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_RCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_Glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "model_RCNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
    "model_RCNN.summary()\n",
    "model_RCNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "predicted = model_RCNN.predict(X_test_Glove)\n",
    "predicted = np.argmax(predicted, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.69       319\n",
      "           1       0.62      0.72      0.67       389\n",
      "           2       0.75      0.55      0.64       394\n",
      "           3       0.57      0.66      0.61       392\n",
      "           4       0.63      0.68      0.66       385\n",
      "           5       0.76      0.67      0.71       395\n",
      "           6       0.85      0.76      0.80       390\n",
      "           7       0.75      0.82      0.78       396\n",
      "           8       0.96      0.77      0.86       398\n",
      "           9       0.88      0.90      0.89       397\n",
      "          10       0.98      0.89      0.93       399\n",
      "          11       0.78      0.83      0.80       396\n",
      "          12       0.49      0.62      0.55       393\n",
      "          13       0.83      0.82      0.83       396\n",
      "          14       0.81      0.79      0.80       394\n",
      "          15       0.82      0.84      0.83       398\n",
      "          16       0.75      0.74      0.75       364\n",
      "          17       0.99      0.77      0.87       376\n",
      "          18       0.55      0.64      0.59       310\n",
      "          19       0.42      0.51      0.46       251\n",
      "\n",
      "    accuracy                           0.74      7532\n",
      "   macro avg       0.75      0.73      0.74      7532\n",
      "weighted avg       0.75      0.74      0.74      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
